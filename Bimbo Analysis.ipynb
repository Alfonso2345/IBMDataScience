{
    "metadata": {
        "language_info": {
            "codemirror_mode": {
                "name": "ipython", 
                "version": 2
            }, 
            "pygments_lexer": "ipython2", 
            "file_extension": ".py", 
            "name": "python", 
            "version": "2.7.11", 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python"
        }, 
        "kernelspec": {
            "language": "python", 
            "display_name": "Python 2 with Spark 2.1", 
            "name": "python2-spark21"
        }
    }, 
    "nbformat": 4, 
    "cells": [
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "\"\"\" \nPlanning a celebration is a balancing act of preparing just \nenough food to go around without being stuck eating the same leftovers \nfor the next week. The key is anticipating how many guests will come. Grupo Bimbo must \nweigh similar considerations as it strives to meet daily consumer demand for fresh bakery products \non the shelves of over 1 million stores along its 45,000 routes across Mexico.\n\nCurrently, daily inventory calculations are performed by direct delivery sales \nemployees who must single-handedly predict the forces of supply, demand, and hunger based on \ntheir personal experiences with each store. With some breads carrying a one week shelf life, \nthe acceptable margin for error is small.\n\nIn this competition, Grupo Bimbo invites Kagglers to develop a model to accurately \nforecast inventory demand based on historical sales data. Doing so will make sure consumers \nof its over 100 bakery products aren\u2019t staring at empty shelves, while also reducing the \namount spent on refunds to store owners with surplus product unfit for sale.\n\nThings to note:\n\n* There may be products in the test set that don't exist in the train set. \n        This is the expected behavior of inventory data, since there are new products being sold all the time. \n        Your model should be able to accommodate this.\n\n* There are duplicate Cliente_ID's in cliente_tabla, which means one Cliente_ID \n        may have multiple NombreCliente that are very similar. This is due to the NombreCliente \n        being noisy and not standardized in the raw data, so it is up to you to decide how to clean up \n        and use this information. \n        \n* The adjusted demand (Demanda_uni_equil) is always >= 0 since demand should be \n        either 0 or a positive value. The reason that Venta_uni_hoy - Dev_uni_proxima sometimes \n        has negative values is that the returns records sometimes carry over a few weeks.\n\nData fields\n\n* Semana \u2014 Week number (From Thursday to Wednesday)\n* Agencia_ID \u2014 Sales Depot ID\n* Canal_ID \u2014 Sales Channel ID\n* Ruta_SAK \u2014 Route ID (Several routes = Sales Depot)\n* Cliente_ID \u2014 Client ID\n* NombreCliente \u2014 Client name\n* Producto_ID \u2014 Product ID\n* NombreProducto \u2014 Product Name\n* Venta_uni_hoy \u2014 Sales unit this week (integer)\n* Venta_hoy \u2014 Sales this week (unit: pesos)\n* Dev_uni_proxima \u2014 Returns unit next week (integer)\n* Dev_proxima \u2014 Returns next week (unit: pesos)\n* Demanda_uni_equil \u2014 Adjusted Demand (integer) (This is the target you will predict)\n\n\"\"\"", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# Count the elements with the label 'SIN NOMBRE' & 'NO IDENTIFICADO'\n#df_TrainSpk0.filter(df_TrainSpk0.NombreCliente.isin(['SIN NOMBRE', 'NO IDENTIFICADO'])).count()\n\n# Erase all the elements with the label 'SIN NOMBRE' & 'NO IDENTIFICADO'\n#df_ClientSpk = df_ClientSpk.filter(df_ClientSpk.NombreCliente != 'SIN NOMBRE')\n#df_ClientSpk = df_ClientSpk.filter(df_ClientSpk.NombreCliente != 'NO IDENTIFICADO')\n#df_TrainSpk01 = df_TrainSpk.groupby(['Semana', 'Producto_ID']).agg({\"Venta_hoy\"}).collect()\n#df_TrainSpk01 = df_TrainSpk.describe()\n\n# Sales/devolution dataframe\n#df_SD = df_TrainSpk.select([\"Semana\", \"Dev_uni_proxima\", \"Venta_uni_hoy\"])\n\n#df_AvgDevProductWeek = df_DevProductWeek.groupby([\"Semana\"]).agg({\"Dev_uni_proxima\":\"avg\"}).collect()\n#df_DevProductWeek = df_DevProductWeek.groupby([\"Semana\"]).agg({\"Dev_uni_proxima\":\"sum\"}).collect()\n#df_ProductsWeek = df_ProductsWeek.groupby([\"Semana\"]).count().collect()\n\n#df_DevWeek = df_SD.groupby([\"Semana\"]).agg({\"Dev_uni_proxima\":\"sum\"}).collect()\n#df_SalWeek = df_SD.groupby([\"Semana\"]).agg({\"Venta_uni_hoy\": \"sum\"}).collect()\n\n#df_PercDevolutions = df_TopAgencies.rdd.map(lambda x: (float(x.Devolucion_unidades)/(x.Devolucion_unidades + x.Venta_unidades)) * 100).collect()\n\n#df_ASG00 = df_TrainSpk02.groupby([\"Semana\",\"Agencia_ID\"]).agg({\"Venta_uni_hoy\": \"sum\"}).collect()\n#df_ASG01 = df_AS.groupby([\"Agencia_ID\"]).agg({\"Venta_uni_hoy\": \"sum\"}).collect()\n\n#pd999 = pd_TopAgencies.assign(Perc_Devolutions = lambda x: x.Devolucion_unidades/(x.Devolucion_unidades + x.Venta_unidades) * 100)\n\n#pd_TrainSpk01.plot(x = 'Semana', y = 'Total Devolutions(units)', kind = 'bar', figsize = [15, 5], title = 'Dev Products by Week')\n#pd_TrainSpk01.plot(x = 'Semana', y = 'Total Sales(units)', kind = 'bar', figsize = [15, 5], title = 'Total Products by Week')\n#pd_TrainSpk01.plot(x = 'Semana', y = 'Total Sales(pesos)', kind = 'bar', figsize = [20, 5], title = 'Total Sales by Week')\n\n#df_ASG01 = spark.createDataFrame(df_ASG01)\n#df_ADG01 = df_ASG01.selectExpr(\"Agencia_ID as Agencia\")", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# The code was removed by DSX for sharing.", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# Print the schema for each data frame\n\nprint(\"\\nClient Schema:\")\ndf_ClientSpk.printSchema()\n\nprint(\"\\nProduct Schema:\")\ndf_ProductSpk.printSchema()\n\nprint(\"\\nTown Schema:\")\ndf_TownSpk.printSchema()\n\nprint(\"\\nTest Schema:\")\ndf_TestSpk.printSchema()\n\nprint(\"\\nTrain Schema:\")\ndf_TrainSpk.printSchema()", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# Set the conditions for the union \nCondClient = [df_TrainSpk.Cliente_ID == df_ClientSpk.Cliente_ID]\nCondProduct = [df_TrainSpk.Producto_ID == df_ProductSpk.Producto_ID]\nCondLocation = [df_TrainSpk.Agencia_ID == df_TownSpk.Agencia_ID]\n\n# Do the join with the above conditions\n\ndf_TrainSpk0 = df_TrainSpk.join(df_ClientSpk, CondClient).drop(df_ClientSpk.Cliente_ID)\ndf_TrainSpk0 = df_TrainSpk0.join(df_ProductSpk, CondProduct).drop(df_ProductSpk.Producto_ID)\ndf_TrainSpk0 = df_TrainSpk0.join(df_TownSpk, CondLocation).drop(df_TownSpk.Agencia_ID)", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# Show the result from the above union\n\ndf_TrainSpk0.take(1)", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# Group the df by semana at the final we'll have the total prodocut sold by week and the total devolutions by week\n\ndf_TrainSpk01 = df_TrainSpk0.select([\"Semana\", \"Dev_uni_proxima\",\"Dev_proxima\", \"Venta_uni_hoy\", \"Venta_hoy\"])\ndf_TrainSpk01 = df_TrainSpk01.groupby([\"Semana\"]).agg({\"Dev_uni_proxima\":\"sum\", \"Dev_proxima\" : \"sum\", \"Venta_uni_hoy\": \"sum\", \"Venta_hoy\": \"sum\"}).collect()", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# In order to make a good visualization, we going to convert the Spark df to a Pandas df\n\npd_TrainSpk01 = pd.DataFrame(df_TrainSpk01)\nprint(pd_TrainSpk01)", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "pd_TrainSpk01.columns = ['Semana', 'Devoluciones(pesos)', 'Venta(unidades)', 'Ventas(pesos)', 'Devolucion(unidades)']", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "print(\"**********************************************\")\nprint(pd_TrainSpk01)", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "pd_TrainSpk01 = pd_TrainSpk01.sort_values(by = ['Semana'], ascending = True)\nprint(pd_TrainSpk01)", 
            "outputs": []
        }, 
        {
            "metadata": {
                "scrolled": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# ****************************************************************************************************\n# The next is the summary sales(units, pesos) and devolutions(units, pesos & % of devolution) by week\n\npd_TrainSpk01.plot(x = 'Semana', kind = 'bar', figsize = [30, 18], subplots = True)", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# ********************************************************************************************************\n# ********************************************************************************************************\n# ********************************************************************************************************\n# In order to know the best agencies we will define the df_AS = dataframe Agency Sales\n\ndf_AS = df_TrainSpk0", 
            "outputs": []
        }, 
        {
            "metadata": {
                "scrolled": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# df_ASG = dataframe Agency Sales Group, data grouped by Agencia with the total of sales by units and pesos and the devolutions by units\n\ndf_ASG01 = df_AS.groupby([\"Agencia_ID\"]).agg({\"Venta_uni_hoy\": \"sum\", \"Dev_uni_proxima\": \"sum\", \"Venta_hoy\": \"sum\", \"Dev_proxima\": \"sum\"}).collect()", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "pd_ASG01 = pd.DataFrame(df_ASG01)", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "pd_ASG01.columns = ['Agencia', 'Devolucion_pesos', 'Venta_unidades','Venta_pesos', 'Devolucion_unidades']", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# Sow the pandas dataframe with the columns renamed\n\npd_ASG01", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# pd_ASG02: pandas dataframe sorted by sales - units\n# pd_ASG03: pandas dataframe sorted by sales - money\n\npd_ASG02 = pd_ASG01.sort_values(by = ['Venta_unidades'], ascending = False)\npd_ASG03 = pd_ASG01.sort_values(by = ['Venta_pesos'], ascending = False)", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# Plot the sorted pandas data frames\n\npd_ASG02.plot(x = 'Agencia', y = 'Venta_unidades', kind = 'bar', figsize = [20, 7], title = 'Venta por agencia - unidades')\npd_ASG03.plot(x = 'Agencia', y = 'Venta_pesos', kind = 'bar', figsize = [20, 7], title = 'Venta por agencia - pesos')", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# Following the 80/20 rule\n# The 20 % of the agencies will generate the 80 % of the sales - units & money\n\nTotalAgency = len(pd_ASG01)\nHundredPercentage = 1\nTwentyPercentage = 0.2\n\nTopAgencies = int((TwentyPercentage * TotalAgency)/HundredPercentage)\n\npd_TopAgenciesUnits = pd_ASG02.head(TopAgencies)\npd_TopAgenciesMoney = pd_ASG03.head(TopAgencies)\n\nprint(\"Total agencies: \" + str(TotalAgency))\nprint(\"20 % top agencies: \" + str(TopAgencies))", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# In the two graphics we can see:\n# - Top agencies by sales(units)\n# - Top agencies by sales(pesos)\n\npd_TopAgenciesUnits.plot(x = 'Agencia', y = 'Venta_unidades', kind = 'bar', figsize = [20, 7], title = 'Agencias con mas ventas - unidades')\npd_TopAgenciesMoney.plot(x = 'Agencia', y = 'Venta_pesos', kind = 'bar', figsize = [20, 7], title = 'Agencias con mas ventas - pesos')", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "df_TopAgenciesUnits = spark.createDataFrame(pd_TopAgenciesUnits)\ndf_TopAgenciesMoney = spark.createDataFrame(pd_TopAgenciesMoney)", 
            "outputs": []
        }, 
        {
            "metadata": {
                "scrolled": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "df_TopAgenciesUnits.show()\ndf_TopAgenciesMoney.show()", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# **********************************************************************************************************************************\nParametersUnits = (df_TopAgenciesUnits.Devolucion_unidades/(df_TopAgenciesUnits.Venta_unidades + df_TopAgenciesUnits.Devolucion_unidades))*100\ndf_TopAgenciesUnits = df_TopAgenciesUnits.withColumn(\"Porc_Devolucion_unidades\", ParametersUnits)\n\n# **********************************************************************************************************************************\n\nParametersMoney = (df_TopAgenciesMoney.Devolucion_pesos/(df_TopAgenciesMoney.Venta_pesos + df_TopAgenciesMoney.Devolucion_pesos))*100\ndf_TopAgenciesMoney = df_TopAgenciesMoney.withColumn(\"Porc_Devolucion_pesos\", ParametersMoney)", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "df_TopAgenciesUnits.show()\ndf_TopAgenciesMoney.show()", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "pd_TopAgenciesUnits = df_TopAgenciesUnits.toPandas()\npd_TopAgenciesMoney = df_TopAgenciesMoney.toPandas()", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "pd_TopAgenciesUnits.head(3)", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "pd_TopAgenciesMoney.head(3)", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# ********************************************************************************************************\n# ********************************************************************************************************\n# Top agencies in sales - units\n# Plot by agency the sales in units, the sales in pesos, the devolutions in units and the devolutions in %\n# ********************************************************************************************************\n# ********************************************************************************************************\n\npd_TopAgenciesUnits.plot.bar(x = 'Agencia', subplots = True, sort_columns=False, figsize = [30, 20])", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# ********************************************************************************************************\n# ********************************************************************************************************\n# Top agencies in sales - pesos\n# Plot by agency the sales in units, the sales in pesos, the devolutions in units and the devolutions in %\n# ********************************************************************************************************\n# ********************************************************************************************************\n\npd_TopAgenciesMoney.plot.bar(x = 'Agencia', subplots = True, sort_columns=False, figsize = [30, 20])", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# Set the conditions for the union \n\nCondLocationUnits = [df_TopAgenciesUnits.Agencia == df_TownSpk.Agencia_ID]\nCondLocationMoney = [df_TopAgenciesMoney.Agencia == df_TownSpk.Agencia_ID]\n\n# Do the union with the above conditions\n\ndf_TopAgenciesUnits = df_TopAgenciesUnits.join(df_TownSpk, CondLocationUnits).drop(df_TownSpk.Agencia_ID)\ndf_TopAgenciesMoney = df_TopAgenciesMoney.join(df_TownSpk, CondLocationMoney).drop(df_TownSpk.Agencia_ID)", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "df_TopAgenciesUnits.show()\ndf_TopAgenciesMoney.show()", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "df_TopAgenciesUnitsTown = df_TopAgenciesUnits.groupby([\"State\"]).agg({\"Venta_unidades\":\"sum\", \"Devolucion_unidades\": \"sum\", \"Agencia\":\"count\"}).collect()\ndf_TopAgenciesMoneyTown = df_TopAgenciesMoney.groupby([\"State\"]).agg({\"Venta_pesos\":\"sum\", \"Devolucion_pesos\": \"sum\", \"Agencia\":\"count\"}).collect()", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "pd_TopAgenciesUnitsTown = pd.DataFrame(df_TopAgenciesUnitsTown)\npd_TopAgenciesMoneyTown = pd.DataFrame(df_TopAgenciesMoneyTown)", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "print(pd_TopAgenciesUnitsTown.head(3))\nprint(pd_TopAgenciesMoneyTown.head(3))", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "pd_TopAgenciesUnitsTown.columns = ['Estado', 'No Agencias','Venta_unidades', 'Devolucion_unidades']\npd_TopAgenciesUnitsTown = pd_TopAgenciesUnitsTown.sort_values(by = ['Venta_unidades'], ascending = False)\n\npd_TopAgenciesMoneyTown.columns = ['Estado', 'No Agencias','Devolucion_pesos', 'Venta_pesos']\npd_TopAgenciesMoneyTown = pd_TopAgenciesMoneyTown.sort_values(by = ['Venta_pesos'], ascending = False)", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "df_TopAgenciesUnitsTown = spark.createDataFrame(pd_TopAgenciesUnitsTown)\ndf_TopAgenciesMoneyTown = spark.createDataFrame(pd_TopAgenciesMoneyTown)", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "df_TopAgenciesUnitsTown.show()\ndf_TopAgenciesMoneyTown.show()", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "ParametersUnits_ = (df_TopAgenciesUnitsTown.Devolucion_unidades/(df_TopAgenciesUnitsTown.Venta_unidades + df_TopAgenciesUnitsTown.Devolucion_unidades))*100\ndf_TopAgenciesUnitsTown = df_TopAgenciesUnitsTown.withColumn(\"Porc_Devolucion_unidades\", ParametersUnits_)\n\nParametersMoney_ = (df_TopAgenciesMoneyTown.Devolucion_pesos/(df_TopAgenciesMoneyTown.Venta_pesos + df_TopAgenciesMoneyTown.Devolucion_pesos))*100\ndf_TopAgenciesMoneyTown = df_TopAgenciesMoneyTown.withColumn(\"Porc_Devolutions_pesos\", ParametersMoney_)", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "df_TopAgenciesUnitsTown.show()\ndf_TopAgenciesMoneyTown.show()", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "pd_TopAgenciesUnitsTown = df_TopAgenciesUnitsTown.toPandas()\npd_TopAgenciesMoneyTown = df_TopAgenciesMoneyTown.toPandas()", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "pd_TopAgenciesUnitsTown = pd_TopAgenciesUnitsTown.sort_values(by = ['Venta_unidades'], ascending = False)\npd_TopAgenciesMoneyTown = pd_TopAgenciesMoneyTown.sort_values(by = ['Venta_pesos'], ascending = False)", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# ********************************************************************************************************\n# ********************************************************************************************************\n# Plot by state the numer of agencies, the sales in units, the sales in pesos, the devolutions in units \n# and the devolutions in %\n# ********************************************************************************************************\n# ********************************************************************************************************\n\npd_TopAgenciesUnitsTown.plot.bar(x = 'Estado', subplots = True, figsize = [30, 20])", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# ********************************************************************************************************\n# ********************************************************************************************************\n# Plot by state the numer of agencies the sales in units, the sales in pesos, the devolutions in units \n# and the devolutions in %\n# ********************************************************************************************************\n# ********************************************************************************************************\n\npd_TopAgenciesMoneyTown.plot.bar(x = 'Estado', subplots = True, figsize = [30, 20])", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "df_CanalID = df_TrainSpk0", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# df_CanalId0: dataframe grouped by Agencia_ID & Canal_ID\n# df_CanalId1: dataframe grouped by Canal_ID\n\ndf_CanalID0 = df_CanalID.groupby([\"Agencia_ID\",\"Canal_ID\"]).agg({\"Venta_uni_hoy\":\"sum\", \"Venta_hoy\":\"sum\",\"Dev_uni_proxima\":\"sum\", \"Dev_proxima\":\"sum\"}).collect()\ndf_CanalID1 = df_CanalID.groupby([\"Canal_ID\"]).agg({\"Venta_uni_hoy\":\"sum\", \"Venta_hoy\":\"sum\", \"Dev_uni_proxima\":\"sum\", \"Dev_proxima\":\"sum\"}).collect()", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "df_CanalID0 = spark.createDataFrame(df_CanalID0)\ndf_CanalID1 = spark.createDataFrame(df_CanalID1)", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "pd_CanalID1 = df_CanalID1.toPandas()", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "pd_CanalID1 = pd_CanalID1.sort_values(by = ['sum(Venta_hoy)'], ascending = False)", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# Plot:\n# - Devolutions by channel (units & pesos)\n# - Sales by channel (units & pesos)\n# the most important channel is the 1 & second the place is for the 2\n\npd_CanalID1.plot.bar(x = 'Canal_ID', subplots = True, figsize = [30, 20])", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# df_CanalID2: data frame 2 canal Id \n# We're selecting the two most important channels according to sales in pesos\n\nCondChannels = df_CanalID0.Canal_ID <= 2\n\ndf_CanalID2 = df_CanalID0.where(CondChannels).collect()", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "df_CanalID2 = spark.createDataFrame(df_CanalID2)", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "TopAgenciesbyMoney = df_TopAgenciesMoney.select([\"Agencia\"])\nTopAgenciesbyUnits = df_TopAgenciesMoney.select([\"Agencia\"])", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# Conditions for the join\n# 00 Top agencies in relation with the sales in mxn (pesos)\n# 01 Top agencies in relation with the sales in units\n\nTopAgenciesCanalId_Cond00 = [df_CanalID2.Agencia_ID == TopAgenciesbyMoney.Agencia]\nTopAgenciesCanalId_Cond01 = [df_CanalID2.Agencia_ID == TopAgenciesbyUnits.Agencia]\n\n# Execute the join with the above conditions\n# df_CanalID3 --> top agencies by money\n# df_CanalID4 --> top agencies by units\n\ndf_CanalID3 = df_CanalID2.join(TopAgenciesbyMoney, TopAgenciesCanalId_Cond00).drop(TopAgenciesbyMoney.Agencia)\ndf_CanalID4 = df_CanalID2.join(TopAgenciesbyUnits, TopAgenciesCanalId_Cond01).drop(TopAgenciesbyUnits.Agencia)", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# Conditions for the join\n# 00 Top agencies in relation with the sales in mxn (pesos)\n# 01 Top agencies in relation with the sales in units\n\nTopAgenciesCanalId_Cond00 = [df_CanalID2.Agencia_ID == TopAgenciesbyMoney.Agencia]\nTopAgenciesCanalId_Cond01 = [df_CanalID2.Agencia_ID == TopAgenciesbyUnits.Agencia]\n\n# Execute the join with the above conditions\n# df_CanalID3 --> top agencies by money\n# df_CanalID4 --> top agencies by units\n\ndf_CanalID3 = df_CanalID2.join(TopAgenciesbyMoney, TopAgenciesCanalId_Cond00).drop(TopAgenciesbyMoney.Agencia)\ndf_CanalID4 = df_CanalID2.join(TopAgenciesbyUnits, TopAgenciesCanalId_Cond01).drop(TopAgenciesbyUnits.Agencia)", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# Create the pandas dataframe in order to plot them\n\npd_CanalID3 = df_CanalID3.toPandas()\npd_CanalID4 = df_CanalID4.toPandas()", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# df_CanalID3 --> top agencies by money with channels 1 or 2\n\npd_CanalID3 = pd_CanalID3.sort_values(by = ['Agencia_ID'], ascending = False)", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# df_CanalID4 --> top agencies by units with channels 1 or 2\n# At this point we know the top agencies with channel 1 or 2 and the toal sales(unit & money) by channel\n\npd_CanalID4 = pd_CanalID4.sort_values(by = ['Agencia_ID'], ascending = False)", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# By we'll continue with the product analysis in order to know the top products \n# Later we'll come back to our top agencies in order to know the top products in the top agencies \n# and check if the top products match with the products of the top agencies", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# The first step si to preprocess the df products in order to know the brand, short name and pieces\n# Is easier do it with pandas than Spark\n\npd_ProductSpk = df_ProductSpk.toPandas()", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "pd_ProductSpk['ShortName'] = pd_ProductSpk.NombreProducto.str.extract('^(\\D*)').astype('string')\npd_ProductSpk['Brand'] = pd_ProductSpk.NombreProducto.str.extract('^.+\\s(\\D+) \\d+$').astype('string')\npd_ProductSpk['Pieces'] =  pd_ProductSpk.NombreProducto.str.extract('(\\d+)p ').astype('float')", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "pd_ProductSpk.head()", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# df_Products: souce dataframe for products\n\ndf_Products = spark.createDataFrame(pd_ProductSpk)", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "df_Products1 = df_Products.groupby([\"ShortName\"]).count().collect()", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "df_Products1 = spark.createDataFrame(df_Products1)", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "df_Products1.show()", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "", 
            "outputs": []
        }
    ], 
    "nbformat_minor": 1
}